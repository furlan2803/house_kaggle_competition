# Ponderada Kaggle

# Compreens√£o do desafio - House Kaggle

Ao examinar o c√≥digo, entende-se que o desafio consiste em prever os pre√ßos de resid√™ncias em Ames, Iowa, com base em 79 vari√°veis que abrangem diversos aspectos relacionados √†s pr√≥prias habita√ß√µes e seus arredores. Essas vari√°veis englobam desde o n√∫mero de quartos e o tamanho do quintal at√© a localiza√ß√£o da casa, considerando elementos como a proximidade de uma ferrovia ou a exist√™ncia de um parque nas proximidades. Dada a complexidade desses fatores e a n√£o evid√™ncia de todos os seus impactos, o objetivo √© criar um modelo capaz de fornecer uma faixa de valores para essas resid√™ncias com base em suas caracter√≠sticas espec√≠ficas.

# M√©todos de treinamento - Essemble 

No c√≥digo fornecido no arquivo "houses_kaggle_competition.ipynb", est√£o dispon√≠veis oito m√©todos de treinamento. Abaixo, s√£o listados juntamente com breve descri√ß√£o de seu funcionamentos.

## √Årvore de Decis√£o

√Årvore de decis√£o √© um modelo que toma decis√µes com base em regras de decis√£o hier√°rquicas. Cada n√≥ da √°rvore representa uma condi√ß√£o sobre um atributo e cada ramo representa o resultado dessa condi√ß√£o. As folhas da √°rvore cont√™m a predi√ß√£o. Os par√¢metros mais relevantes para o m√©todo s√£o, max_depth (profundidade m√°xima da √°rvore), min_samples_leaf (n√∫mero m√≠nimo de amostras em uma folha).

## KNN (K-Nearest Neighbors)

KNN √© um algoritmo de aprendizado supervisionado que classifica os pontos de dados com base na proximidade com os vizinhos mais pr√≥ximos. A predi√ß√£o √© feita pela maioria das classes dos vizinhos mais pr√≥ximos. O m√©todo √© sens√≠vel √† escala dos recursos, computacionalmente caro para grandes conjuntos de dados.

## Ridge (Regress√£o Linear com Regulariza√ß√£o L2)

Regress√£o Ridge √© uma extens√£o da regress√£o linear que adiciona um termo de regulariza√ß√£o L2 √† fun√ß√£o de custo. Isso ajuda a evitar overfitting, especialmente quando h√° multicolinearidade entre os recursos. Por√©m, √© sens√≠vel √† escala dos recursos.

## SVM (Support Vector Machine)

M√°quinas de Vetores de Suporte s√£o utilizadas para problemas de classifica√ß√£o e regress√£o. Buscam encontrar o hiperplano que melhor separa as classes ou se ajusta aos dados de regress√£o. M√©todo sens√≠vel √† escala dos recursos, escolha do kernel influencia o desempenho.

## Floresta Aleat√≥ria

Floresta Aleat√≥ria √© um conjunto de √°rvores de decis√£o treinadas de forma independente. A predi√ß√£o √© a m√©dia ou a moda das previs√µes das √°rvores individuais. Uma das vantagens do m√©todo √© que reduz overfitting e captura rela√ß√µes n√£o lineares.

## Boosted Trees (AdaBoost e Gradient Boosting)

Os m√©todos de Boosting combinam modelos fracos para formar um modelo forte. AdaBoost foca em exemplos mal classificados, enquanto Gradient Boosting ajusta os erros residuais de modelos anteriores. Uma das vantagens do m√©todo √© ser menos propenso a overfitting.

## Stacking

Stacking combina as previs√µes de v√°rios modelos de base usando um meta-modelo. Pode melhorar o desempenho combinando diferentes perspectivas dos modelos individuais, por√©m, pode ser computacionalmente intensivo.

## XGBoost

Implementa√ß√£o otimizada de Gradient Boosting que oferece efici√™ncia computacional, regulariza√ß√£o, tratamento de dados ausentes e paralelismo. Eficiente para grandes conjuntos de dados e suporte a early stopping.

# Diferen√ßa de treinamento Te√≥rica - Essemble 

Apresenta-se abaixo as principais diferen√ßas de treinamento entre os m√©todos de Ensemble:

## Diversidade dos modelos

- **√Årvores de decis√£o, KNN, Ridge, SVM:** Modelos individuais que n√£o se baseiam em outros para fazer previs√µes.
- **Random Forest e Boosted Trees:** Constroem uma cole√ß√£o de modelos fracos (√Årvores de decis√£o) sequencialmente, onde cada modelo aprende a corrigir os erros do anterior.
- **Stacking:** Combina as previs√µes de v√°rios modelos individuais (como gboost, adaboost, ridge, svm) para gerar uma previs√£o final.
- **XGBoost:** Utiliza √°rvores de decis√£o como modelos base.

## Natureza do aprendizado:

- **√Årvores de decis√£o, KNN, Ridge, SVM:** Aprendem independentemente a partir do conjunto de dados completo.
- **Random Forest e Boosted Trees:** Cada modelo base √© treinado em uma subamostra aleat√≥ria do conjunto de dados (bagging) ou a partir da distribui√ß√£o de erros do modelo anterior (boosting).
- **Stacking:** Os modelos base s√£o treinados individualmente no conjunto de dados completo, e ent√£o um meta-modelo (como Regress√£o Linear) √© treinado para combinar suas previs√µes.
- **XGBoost:** √â um m√©todo de boosting.

## Configura√ß√£o dos hiperpar√¢metros:

- **√Årvores de decis√£o, KNN, Ridge, SVM:** Cada modelo tem seus pr√≥prios hiperpar√¢metros que precisam ser ajustados (por exemplo, profundidade m√°xima da √°rvore, n√∫mero de vizinhos, constante de regulariza√ß√£o).
- **Random Forest e Boosted Trees:** Al√©m dos hiperpar√¢metros dos modelos base, tamb√©m h√° hiperpar√¢metros que controlam a constru√ß√£o do ensemble (por exemplo, n√∫mero de √°rvores, taxa de aprendizagem).
- **Stacking:** Cada modelo base tem seus pr√≥prios hiperpar√¢metros, e o meta-modelo tamb√©m pode ter seus pr√≥prios (por exemplo, pesos atribu√≠dos aos modelos base).
- **XGBoost:** Possui uma variedade de hiperpar√¢metros que podem ser ajustados, incluindo o n√∫mero de √°rvores, a taxa de aprendizagem, o tamanho do passo e o coeficiente de regulariza√ß√£o.

## Complexidade de treinamento:

- **√Årvores de decis√£o, KNN, Ridge, SVM:** Relativamente simples e r√°pidos de treinar.
- **Random Forest e Boosted Trees:** Podem ser mais lentos de treinar devido √† constru√ß√£o sequencial de v√°rios modelos.
- **Stacking:** Exige o treinamento de v√°rios modelos individuais e depois o meta-modelo, podendo ser o mais lento.
- **XGBoost:** Pode ser mais lento de treinar do que outros m√©todos de ensemble, pois usa um algoritmo de otimiza√ß√£o gradiente.

## Interpretabilidade:

- **√Årvores de decis√£o, KNN, Ridge, SVM:** F√°ceis de interpretar individualmente.
- **Random Forest e Boosted Trees:** Dif√≠ceis de interpretar devido √† natureza combinada das previs√µes.
- **Stacking:** A interpretabilidade depende do meta-modelo escolhido.
- **XGBoost:** Dif√≠cil de interpretar, pois √© um m√©todo de boosting.


# Diferen√ßa de treinamento Pr√°tica - Essemble 

## √Årvore de Decis√£o:

- Utiliza DecisionTreeRegressor do scikit-learn.
- Hiperpar√¢metros ajustados: max_depth e min_samples_leaf.

## KNN (K-Nearest Neighbors):

- Utiliza KNeighborsRegressor do scikit-learn.
- N√£o especifica hiperpar√¢metros diretamente.

## Ridge (Regress√£o Linear com Regulariza√ß√£o L2):

- Utiliza Ridge do scikit-learn.
- Pode ser treinado tanto com o target original quanto com o log do target.

## SVM (Support Vector Machine):

- Utiliza SVR do scikit-learn com um kernel linear.

## Floresta Aleat√≥ria:

- Utiliza RandomForestRegressor do scikit-learn.
- Hiperpar√¢metros ajustados: max_depth e min_samples_leaf.

## Boosted Trees (AdaBoost e Gradient Boosting):

- Para AdaBoost, utiliza AdaBoostRegressor com √°rvores de decis√£o.
- Para Gradient Boosting, utiliza GradientBoostingRegressor.

## Stacking:

- Utiliza VotingRegressor ou StackingRegressor do scikit-learn.
- Combina os modelos individuais (Gradient Boosting, AdaBoost, Ridge, SVM) com pesos iguais no primeiro caso e usando um meta-modelo Linear Regression no segundo.

## XGBoost:

- Pode ser integrado a um pipeline do scikit-learn ou treinado diretamente com a biblioteca XGBoost.
- Na segunda op√ß√£o, utiliza conjuntos de treino/valida√ß√£o e crit√©rio de parada antecipada.

**Observa√ß√£o:** Todos os m√©todos utilizam Cross-Validation para avalia√ß√£o.

# Extra

Com o objetivo de aprimorar o c√≥digo, prop√µe-se um novo m√©todo de pr√©-processamento que incorpora uma descri√ß√£o mais abrangente do conjunto de dados. Al√©m disso, demonstra-se a abordagem espec√≠fica para lidar com dados nulos em cada coluna do conjunto.

A visualiza√ß√£o pode ser feita no seguinte documento <a href="./entendimento_e_preprocessamento.ipynb">entendimento_e_preprocessamento.ipynb</a>

# Sugest√µes de mudan√ßa no c√≥digo 

S√£o fornecidos exemplos de como aplicar os c√≥digos com base nas sugest√µes apresentadas.

## √Årvores de decis√£o

Aplicar t√©cnica de sele√ß√£o de atributos 'SelectKBest' para reduzir a dimensionalidade dos dados e utilizar algoritmo de busca por espa√ßo de hiperpar√¢metros GridSearchCV. O objetivo √© melhorar a performance e a interpretabilidade do modelo al√©m de encontrar os melhores valores de hiperpar√¢metros.

```Python
k_best = [5, 10, 15, 20]

param_grid = {
    'decisiontreeregressor__max_depth': [10, 20, 30, 40],
    'decisiontreeregressor__min_samples_leaf': [5, 10, 15, 20]
}

selector = SelectKBest(score_func=f_regression)

model = DecisionTreeRegressor()
pipe = Pipeline([
    ('preproc', preproc),
    ('selector', selector),
    ('model', model)
])

grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)
grid_search.fit(X, y_log)

selected_features = selector.get_support(indices=True)

best_model = grid_search.best_estimator_
score = cross_val_score(best_model, X, y_log, cv=5, scoring='neg_mean_squared_error')

print("Melhores par√¢metros:", grid_search.best_params_)
print("Atributos selecionados:", selected_features)
print("RMSE m√©dio:", abs(score.mean())**0.5)
```

## KNN

Utilizar algoritmo de vizinhan√ßa BallTree e t√©cnica de sele√ß√£o de atributos para reduzir a dimensionalidade dos dados. O objetivo √© melhorar a performance e a interpretabilidade do modelo.

```Python
k_best = [5, 10, 15, 20]

param_grid = {'kneighborsregressor__n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30]}

selector = SelectKBest(score_func=f_regression)

ball_tree = BallTree()
model = KNeighborsRegressor()
pipe_knn = make_pipeline(preproc, selector, ball_tree, model)

scores = cross_val_score(pipe_knn, X, y_log, cv=5, scoring=make_scorer(rmse, greater_is_better=False))

param_grid = {
    'kneighborsregressor__n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30],
    'selectkbest__k': k_best
}

search_knn = GridSearchCV(
    pipe_knn,
    param_grid=param_grid,
    cv=3,
    n_jobs=-1,
    verbose=2,
    scoring=make_scorer(rmse, greater_is_better=False)
)

search_knn.fit(X, y_log)

print("RMSE m√©dio: \n", abs(scores.mean()))
print(f'Melhores par√¢metros {search_knn.best_params_} \n')
print(f'Melhor RMSE {abs(search_knn.best_score_)}')
```

## Ridge

Utilizar algoritmo de otimiza√ß√£o como L-BFGS e t√©cnica de regulariza√ß√£o para evitar o overfitting. O objetivo √© melhorar a performance do modelo no conjunto de dados de teste.

```Python

def objective_function(params, X, y, alpha):
    beta = params.reshape(-1, 1)
    loss = np.mean((y - X @ beta)**2) + alpha * np.sum(beta**2)
    return loss

def gradient_function(params, X, y, alpha):
    beta = params.reshape(-1, 1)
    gradient = -2 * X.T @ (y - X @ beta) + 2 * alpha * beta
    return gradient.flatten()

class RidgeWithLBFGS(Ridge):
    def __init__(self, alpha=1.0, max_iter=100, tol=1e-3):
        super().__init__(alpha=alpha)
        self.max_iter = max_iter
        self.tol = tol

    def fit(self, X, y):
        scaler = StandardScaler()
        X_std = scaler.fit_transform(X)
        initial_params = self.coef_.flatten()

        result = minimize(
            fun=objective_function,
            jac=gradient_function,
            x0=initial_params,
            args=(X_std, y, self.alpha),
            method='L-BFGS-B',
            options={'maxiter': self.max_iter, 'disp': True, 'ftol': self.tol}
        )

        self.coef_ = result.x.reshape(-1, 1)
        return self

cachedir = mkdtemp()
preproc = StandardScaler()
model = RidgeWithLBFGS()

pipe_ridge = make_pipeline(preproc, model, memory=cachedir)
cross_val_score(pipe_ridge, X, y, cv=5, scoring=rmsle).mean()

param_grid = {'ridgewithlbfgs__alpha': np.linspace(0.5, 2, num=20)}

search_ridge = GridSearchCV(
    pipe_ridge,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2,
    scoring=rmse_neg
)

search_ridge.fit(X, y_log);

print(f'Melhores par√¢metros {search_ridge.best_params_}')
print(f'Melhor Score {search_ridge.best_score_}')
```

## SVM

Utilizar kernel com Polynomial e t√©cnica de regulariza√ß√£o para evitar o overfitting. O objetivo √© melhorar a performance do modelo no conjunto de dados de teste.

```Python

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

rmse_neg = make_scorer(rmse, greater_is_better=False)

model_poly = SVR(kernel='poly')
preproc = make_pipeline(StandardScaler(), PCA())
regressor = TransformedTargetRegressor(regressor=model_poly, transformer=StandardScaler())

pipe_svm_poly = make_pipeline(preproc, regressor)

if allow_grid_searching:
    param_grid_poly = {
        'transformedtargetregressor__regressor__C': [0.5, 0.7, 1, 2, 5, 10],
        'transformedtargetregressor__regressor__epsilon': [0.01, 0.05, 0.1, 0.2, 0.5],
        'transformedtargetregressor__regressor__degree': [2, 3, 4], 
    }

    search_svm_poly = GridSearchCV(
        pipe_svm_poly,
        param_grid=param_grid_poly,
        cv=5,
        n_jobs=-1,
        verbose=2,
        scoring=rmse_neg
    )

    search_svm_poly.fit(X, y_log)

    svm_poly_best = search_svm_poly.best_estimator_

    print(f'Melhores par√¢metros {search_svm_poly.best_params_}')
    print(f'Melhor Score {search_svm_poly.best_score_}')

```

## Random Forest

Utilizar um maior n√∫mero de √°rvores para melhorar a precis√£o do modelo e um algoritmo de busca por espa√ßo de hiperpar√¢metros como RandomizedSearchCV. O objetivo √© encontrar os melhores valores para os hiperpar√¢metros do modelo.

```Python 

preproc = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), slice(None)), 
        ('imp', SimpleImputer(strategy='mean'), slice(None))
    ])

model = RandomForestRegressor(max_depth=50, min_samples_leaf=20)
pipe = make_pipeline(preproc, model)

param_dist = {
    'randomforestregressor__n_estimators': [50, 100, 200, 300],  
    'randomforestregressor__max_depth': [10, 20, 30, 40, 50], 
    'randomforestregressor__min_samples_leaf': [5, 10, 20, 30]  
}

random_search = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

random_search.fit(X, y_log)

print("Melhores hiperpar√¢metros:", random_search.best_params_)
score = cross_val_score(random_search.best_estimator_, X, y_log, cv=5, scoring='neg_mean_squared_error')
print("RMSE m√©dio:", (-score.mean())**0.5)
```

## Boosted Trees

Utilizar um n√∫mero maior de √°rvores para melhorar a precis√£o do modelo, uma taxa de aprendizagem menor para evitar o overfitting e um algoritmo de busca por espa√ßo de hiperpar√¢metros, GridSearchCV. O objetivo √© ajudar a melhorar a performance do modelo no conjunto de dados de teste e encontrar os melhores valores para os hiperpar√¢metros.

```Python

def rmse(y_true, y_pred):
    return np.sqrt(((y_true - y_pred) ** 2).mean())

cachedir = mkdtemp()

model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None), n_estimators=100, learning_rate=0.01)
pipe = make_pipeline(FunctionTransformer(func=preproc), model, memory=cachedir)
score = cross_val_score(pipe, X, y_log, cv=5, scoring=rmse)

print("Cross-validation RMSE:")
print(score.std())
print(score.mean())

grid = {
    'adaboostregressor__n_estimators': [50, 100, 150],
    'adaboostregressor__learning_rate': [0.01, 0.1, 0.5]
}

search_ab = GridSearchCV(pipe, grid, scoring='neg_mean_squared_error', cv=5, n_jobs=1, verbose=2)
search_ab.fit(X, y_log)

print(f'Melhores par√¢metros {search_ab.best_params_}')
print(f'Melhor score üëâ {search_ab.best_score_}')

df_cv_results_ = pd.DataFrame(search_ab.cv_results_)
sns.scatterplot(x="param_adaboostregressor__n_estimators", y='mean_test_score', data=df_cv_results_)
sns.scatterplot(x="param_adaboostregressor__learning_rate", y='mean_test_score', data=df_cv_results_)
```

## Stacking

Utilizar um meta-modelo com uma regress√£o linear, usar um n√∫mero maior de modelos base e hiperpar√¢metros de RandomizedSearchCV. O objetivo √© melhorar a precis√£o do modelo e encontrar os melhores valores para os hiperpar√¢metros.

```Python

gboost = GradientBoostingRegressor(n_estimators=100)
ridge = Ridge()
svm = SVR(C=1, epsilon=0.05)
adaboost = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=None))

model = StackingRegressor(
    estimators=[("gboost", gboost), ("adaboost", adaboost), ("ridge", ridge), ("svm_rbf", svm)],
    final_estimator=LinearRegression(),
    cv=5,
    n_jobs=-1
)

pipe_stacking = make_pipeline(preproc, model, memory=cachedir)

param_dist = {
    'stackingregressor__gboost__n_estimators': randint(50, 200),
    'stackingregressor__ridge__alpha': uniform(0.1, 10),
    'stackingregressor__svm_rbf__C': [0.1, 1, 10],
}

random_search = RandomizedSearchCV(
    estimator=pipe_stacking,
    param_distributions=param_dist,
    n_iter=10,  
    scoring='neg_mean_squared_error',  
    cv=5,
    n_jobs=-1
)

random_search.fit(X, y_log)
score = cross_val_score(random_search.best_estimator_, X, y_log, cv=5, scoring=rmse, n_jobs=-1)

print("Melhores Hiperpar√¢metros:", random_search.best_params_)
print("Pontua√ß√£o M√©dia:", score.mean())
print("Desvio Padr√£o:", score.std())
```

## XGBoost

Utilizar um n√∫mero maior de √°rvores para melhorar a precis√£o do modelo, taxa de aprendizagem menor para evitar o overfitting e um algoritmo de early stopping para interromper o treinamento quando o desempenho no conjunto de dados de valida√ß√£o parar de melhorar. O objetivo √© melhorar a performance do modelo no conjunto de dados de teste e evitar o overfitting.

```Python
X_train, X_eval, y_train_log, y_eval_log = train_test_split(X, y_log, random_state=42)

model_xgb = XGBRegressor(
    max_depth=10,
    n_estimators=3000,  
    learning_rate=0.01,  
    early_stopping_rounds=50, 
    eval_metric="rmse", 
    eval_set=[(X_eval, y_eval_log)], 
    verbose=False  
)

pipe_xgb = make_pipeline(YourPreprocessor(), model_xgb)  

param_grid = {
    "xgbregressor__max_depth": [8, 10, 12],
}

grid_search = GridSearchCV(pipe_xgb, param_grid, cv=5, scoring=make_scorer(rmse), n_jobs=-1)
grid_search.fit(X, y_log)

print("Melhores par√¢metros:", grid_search.best_params_)
print("Melhor pontua√ß√£o RMSE:", grid_search.best_score_)
```

